from typing import Optional, Type
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.tools import BaseTool
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI
from openai import OpenAI
import requests
import os

    
class AssistantInput(BaseModel):
    question: str = Field(description="The question to be asked from GPT models.")

class GPT4TAssistant(BaseTool):
    name = "GPT4-Turbo_General_Assistant"
    description = "Use this tool, when the user wants an answer from GPT4 or GPT4-Turbo general assistant.\n" +\
    "This tool accepts one input question:\n" + \
    "[question]\n" + \
    "Don't change the input question from the user and don't change answer from this tool\n."  +\
    "Just pass it through to the user."
    
    args_schema: Type[BaseModel] = AssistantInput

    def _run(self, question: str):
        prompt = ChatPromptTemplate.from_template(
            "You are a general AI assistant. Answer questions with minimal and to the point explanation.\n" +
            "Don't put safety and cultural warnings. Only warn about security." +
            "answer the following question: {question}")
        model = ChatOpenAI(model="gpt-4-turbo-preview")
        output_parser = StrOutputParser()

        chain = prompt | model | output_parser
        return chain.invoke({"question": question})

    async def _arun(self, question: str):
        return self._run(question)


class GPT4TCodeGen(BaseTool):
    name = "GPT4-Turbo_Code_Assistant"
    description = "Use this tool, when the user wants code generated by GPT4 or GPT4-Turbo code assistant.\n" +\
    "This tool accepts one input question:\n" + \
    "[question]\n" + \
    "Don't change the input question from the user and don't change answer from this tool\n."  +\
    "Just pass it through to the user."
    
    args_schema: Type[BaseModel] = AssistantInput

    def _run(self, question: str):
        prompt = ChatPromptTemplate.from_template(
            "You are a code assistant. Answer questions in code with minimal to no explanation.\n" +
            "Put brief one line comments on the code for explanation." +
            "answer the following question: {question}")
        model = ChatOpenAI(model="gpt-4-turbo-preview")
        output_parser = StrOutputParser()

        chain = prompt | model | output_parser
        return chain.invoke({"question": question})

    async def _arun(self, question: str):
        return self._run(question)
    
class GPT35TCodeGen(BaseTool):
    name = "GPT35-Turbo_Code_Assistant"
    description = "Use this tool, when the user wants code generated by GPT3.5 or GPT3.5-Turbo code assistant.\n" +\
    "This tool accepts one input question:\n" + \
    "[question]\n" + \
    "Don't change the input question from the user and don't change answer from this tool\n."  +\
    "Just pass it through to the user."
    
    args_schema: Type[BaseModel] = AssistantInput

    def _run(self, question: str):
        prompt = ChatPromptTemplate.from_template(
            "You are a code assistant. Answer questions in code with minimal to no explanation.\n" +
            "Put brief one line comments on the code for explanation." +
            "answer the following question: {question}")
        model = ChatOpenAI(model="gpt-4-turbo-preview")
        output_parser = StrOutputParser()

        chain = prompt | model | output_parser
        return chain.invoke({"question": question})

    async def _arun(self, question: str):
        return self._run(question)
    

class GeneratorInput(BaseModel):
    prompt: str = Field(description="The prompt for image generation.")

class DalleImageGen(BaseTool):
    name = "Dalle3_Image_Generator"
    description = "Use this tool, when the user wants images generated by Dall-e-3.\n" +\
    "This tool accepts one input prompt:\n" + \
    "[prompt]\n" + \
    "The output is a json blob with image path in it. Pass the output to the user."
    
    args_schema: Type[BaseModel] = GeneratorInput
    model_name: str = "dall-e-3"#,"dall-e-2"
    image_folder: str = "/home/raha/code/Stevens-Courses/BIA_810/genai-stevens/bot/images"
    image_number: int = 0

    def _run(self, prompt: str):
        client = OpenAI()
        image_data = client.images.generate(
                model=self.model_name,
                prompt=prompt,
                size="1024x1024",
                quality="standard",
                n=1,
        )
        image = requests.get(image_data.data[0].url,stream=True)
        if image.status_code == 200:
            image_path = os.path.join(self.image_folder,
                                      f"dall-e-3_{self.image_number}.png")
            self.image_number += 1
            with open(image_path, 'wb') as f:
                for chunk in image:
                    f.write(chunk)
            return {"image_path":image_path}
        return {}

    async def _arun(self, prompt: str):
        return self._run(prompt)
    
class RAGTool(BaseTool):
    name = "RAG_Assistant"
    description = "Use this tool, when the user wants to know anything related to LLM Powered Autonomous Agents.\n" +\
    "The questions could be about agent planing, memory and tools." + \
    "Examples and case studies of such agents and challenges are covered.\n" + \
    "This tool accepts one input question:\n" + \
    "[question]\n" + \
    "Don't change the input question from the user and don't change answer from this tool\n."  +\
    "Just pass it through to the user."
    retriever: Type[VectorStoreRetriever] = None
    llm: Type[ChatOpenAI] = None
    prompt: Type[ChatPromptTemplate] = None

    def __init__(self,retriever,llm,prompt):
        super().__init__()
        self.retriever = retriever
        self.llm = llm
        self.prompt = prompt
    
    args_schema: Type[BaseModel] = AssistantInput


    @staticmethod
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    def _run(self, question: str):
        
        rag_chain = (
            {"context": self.retriever | self.format_docs, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )
        rag_chain.invoke(question)
        
    async def _arun(self, question: str):
        return self._run(question)